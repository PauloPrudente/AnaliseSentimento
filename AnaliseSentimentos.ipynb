{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnaliseSentimentos.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZNhxgQmsmc8dvGy+IxfOX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PauloPrudente/AnaliseSentimento/blob/main/AnaliseSentimentos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8nmckT7ZKiW"
      },
      "source": [
        "https://www.linkedin.com/pulse/realizando-an%C3%A1lise-de-sentimento-partir-coment%C3%A1rios-da-rodrigo-correa/\n",
        "https://www.linkedin.com/pulse/analisando-sentimento-em-notas-de-app-python-parte-2-rodrigo-correa/\n",
        "https://www.linkedin.com/pulse/analisando-sentimento-em-notas-de-app-python-parte-3-rodrigo-correa/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bjXDNQrrZZk"
      },
      "source": [
        "pip install google-play-scraper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM8gtJwRsAfN"
      },
      "source": [
        "#Importando o comando app da library google-play-scraper\n",
        "from google_play_scraper import app"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAMY-G5psJD1"
      },
      "source": [
        "result = app('com.rappi.storekeeper', lang = 'pt', country = 'br')\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upo0pj6LsmSd"
      },
      "source": [
        "#Baixando todos os reviews do app.\n",
        "from google_play_scraper import Sort, reviews_all\n",
        "\n",
        "#Comando para todas as reviews\n",
        "\n",
        "Reviews = reviews_all( 'com.rappi.storekeeper', lang = 'pt', country = 'br', sort = Sort.MOST_RELEVANT, sleep_milliseconds = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW8cxTmLs09_"
      },
      "source": [
        "#Importando pandas.\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#Transformando os dados em um DataFrame para trabalharmos as análises.\n",
        "Reviews_Rappi = pd.DataFrame(Reviews)\n",
        "\n",
        "#Verificando a serie de dados.\n",
        "Reviews_Rappi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3Q3Zfigs7HW"
      },
      "source": [
        "#Importando o nltk e salvando os corpus necessários\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Aplicando uma função para tokenizar por palavra\n",
        "\n",
        "Reviews_Rappi['content'] = Reviews_Rappi.apply(lambda row: nltk.word_tokenize(row['content']), axis=1) # Tokenização dos dados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAHrQ94btFZt"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "language = 'portuguese'\n",
        "\n",
        "#Criando a lista de stopwords\n",
        "stopwords = stopwords.words(language)\n",
        "stopwords = list(set(stopwords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WT0SENltP0p"
      },
      "source": [
        "def remove_stopwords(words):\n",
        "    \"\"\"Remover as Stopwords das palavras tokenizadas\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            new_words.append(word)\n",
        "    return new_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLWpP1HztWnG"
      },
      "source": [
        "def to_lowercase(words):\n",
        "    \"\"\"converter todos os caracteres para lowercase\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdmBrOG8tZ24"
      },
      "source": [
        "def remove_punctuation(words):\n",
        "    \"\"\"remover pontuação\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    \n",
        "\n",
        "    return new_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUQQjS0rtfPB"
      },
      "source": [
        "def normalize(words):\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    \n",
        "    return ' '.join(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVHlgsVtjp-"
      },
      "source": [
        "Reviews_Rappi['content'] = Reviews_Rappi.apply(lambda row: normalize(row['content']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpnOw0nYxOcz"
      },
      "source": [
        "#Importando o Léxico de Palavras com polaridades\n",
        "sentilexpt = open('SentiLex-lem-PT02.txt')\n",
        "\n",
        "#Criando um dicionário de palavras com a respectiva polaridade.\n",
        "dic_palavra_polaridade = {}\n",
        "for i in sentilexpt.readlines():\n",
        "  pos_ponto = i.find('.')\n",
        "  palavra = (i[:pos_ponto])\n",
        "  pol_pos = i.find('POL')\n",
        "  polaridade = (i[pol_pos+7:pol_pos+9]).replace(';', '')\n",
        "  dic_palavra_polaridade[palavra] = polaridade\n",
        "\n",
        "\n",
        "#Verificando o dicionário\n",
        "\n",
        "dic_palavra_polaridade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPzy0_AuzBZZ"
      },
      "source": [
        "#Criando uma função chamada \"Score de Sentimento\" para determinar os #sentimentos associados\n",
        "def Score_sentimento(frase):\n",
        "    frase = frase.lower()\n",
        "    l_sentimento = []\n",
        "    for p in frase.split():\n",
        "        l_sentimento.append(int(dic_palavra_polaridade.get(p, 0)))\n",
        "    score = sum(l_sentimento)\n",
        "    if score > 0:\n",
        "        return 'Pos {} '.format(score)\n",
        "    elif score == 0:\n",
        "        return 'Neu {} '.format(score)\n",
        "    else:\n",
        "        \n",
        "        return 'Neg {}'.format(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PavBu5XzH4Z"
      },
      "source": [
        "#Criando uma função para aplicar um score de sentimento para cada um dos comentários, a partir das palavras positivas e negativas.\n",
        "Reviews_Rappi['sentimento'] = Reviews_Rappi.apply(lambda row: Score_sentimento(row['content']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvCV2lAVzO-o"
      },
      "source": [
        "#Reorganizando o resultado em colunas para posteriormente lançar no modelo\n",
        "Reviews_Rappi['Score_Sentimento'] = Reviews_Rappi['sentimento'].str.slice(-2)\n",
        "Reviews_Rappi['Score_Sentimento'] =Reviews_Rappi['Score_Sentimento'].astype(int)\n",
        "Reviews_Rappi['Sent'] = Reviews_Rappi['sentimento'].str.slice(0,-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq6JoxSKzTJU"
      },
      "source": [
        "#Verificando como ficou a distribuição de comentários a partir do Score de Sentimento Criado.\n",
        "Reviews_Rappi.groupby('Score_Sentimento').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mjw7OzPzWa9"
      },
      "source": [
        "#criando um objeto somente com os comentários\n",
        "content = Reviews_Rappi['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNa-uSSvzb_L"
      },
      "source": [
        "#juntando todos eles para construir a wordcloud - ela tem que estar todo contido numa string\n",
        "all_content = \"\".join(c for c in content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxbqeWgVAxNh"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVtchRL6zium"
      },
      "source": [
        "#importando as libraries necessárias para o wordcloud\n",
        "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
        "#montando um novo dicionário de stopwords\n",
        "stopwords = set(STOPWORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFlbS5ZDzi_U"
      },
      "source": [
        "#Após review das palavras, adicionando alguns termos \"sujeira\" encontrados nas nuvens\n",
        "stopwords.update([\"pra\", \"app\", \"aplicativo\", \"vc\", \"pra\", \"to\", \"os\", \"rappi\", \"vcs\", \"nao\", \"pq\", \"mim\", \"ai\", \"ta\", \"ja\", \"ter\", \"fazer\", \"lá\", \"deu\", \"dado\", \"então\", \"vou\", \"vai\", \"veze\", \"ficar\", \"tá\", \"apena\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkRlKxYVztxq"
      },
      "source": [
        "#Criando o objeto wordcloud com as configs necessárias. Cor escolhida = preta, origem dos dados = all_content\n",
        "wordcloud = WordCloud(stopwords=stopwords,background_color='black', width=1600,height=800).generate(all_content)\n",
        "#configurando forma de apresentação do gráfico e apresentando no notebook.\n",
        "fig, ax = plt.subplots(figsize=(16,8))            \n",
        "ax.imshow(wordcloud, interpolation='bilinear')       \n",
        "ax.set_axis_off()\n",
        "plt.imshow(wordcloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laTecAQ3GyeX"
      },
      "source": [
        "#Criando agora um Dataset apenas com o que vamos usar no modelo. Não preciso incluir nenhum detalhe sobre quem deu o review.\n",
        "Rappi = Reviews_Rappi[['content', 'thumbsUpCount', 'reviewCreatedVersion', 'at', 'score',  'Score_Sentimento', 'Sent']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZEi9RKyGznZ"
      },
      "source": [
        "#Realizando o mesmo processo, porém agora para avaliações negativas - notas 1 e 2\n",
        "Negative = Rappi[Rappi.score < 3]\n",
        "Neg_Content = Negative['content']\n",
        "all_neg_content = \"\".join(c for c in Neg_Content)\n",
        "wordcloud = WordCloud(stopwords=stopwords,\n",
        "                      background_color='orange', width=1600,                            \n",
        "                      height=800).generate(all_neg_content)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))            \n",
        "ax.imshow(wordcloud, interpolation='bilinear')       \n",
        "ax.set_axis_off()\n",
        "plt.imshow(wordcloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvnhwexaHY9c"
      },
      "source": [
        "#Realizando o mesmo procedimento, para avaliações consideradas neutras (Nota = 3)\n",
        "Neutral = Rappi[Rappi.score == 3]\n",
        "Neu_Content = Neutral['content']\n",
        "all_neu_content = \"\".join(c for c in Neu_Content)\n",
        "wordcloud = WordCloud(stopwords=stopwords,\n",
        "                      background_color='blue', width=1600,                            \n",
        "                      height=800).generate(all_neu_content)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))            \n",
        "ax.imshow(wordcloud, interpolation='bilinear')       \n",
        "ax.set_axis_off()\n",
        "plt.imshow(wordcloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaKnNDqjHfW9"
      },
      "source": [
        "#Finalmente, realizando o procedimento para notas chamadas Positivas, (4 e 5)\n",
        "Positive = Rappi[Rappi.score > 3]\n",
        "Pos_Content = Positive['content']\n",
        "all_pos_content = \"\".join(c for c in Pos_Content)\n",
        "wordcloud = WordCloud(stopwords=stopwords,\n",
        "                      background_color='green', width=1600,                            \n",
        "                      height=800).generate(all_pos_content)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))            \n",
        "ax.imshow(wordcloud, interpolation='bilinear')       \n",
        "ax.set_axis_off()\n",
        "plt.imshow(wordcloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd1PiQwQHmpM"
      },
      "source": [
        "# Vetorização (Converter texto e números).\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1000)                \n",
        "data_features = vectorizer.fit_transform(Rappi['content'])\n",
        "\n",
        "\n",
        "data_features = data_features.toarray()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7qacgYGHzcg"
      },
      "source": [
        "labels = Rappi['score'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_a-OR3-H5hE"
      },
      "source": [
        "# Split data into training and testing set.\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_features, labels, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq08QokhIK7Q"
      },
      "source": [
        "# Usando Random Forest para classificar os reviews.\n",
        "# Também calculando o Score Cross Validated.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "forest = RandomForestClassifier(n_estimators=10, n_jobs=4)\n",
        "\n",
        "\n",
        "forest = forest.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(forest)\n",
        "\n",
        "\n",
        "print(np.mean(cross_val_score(forest, data_features, labels, cv=10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQLNW2feI36I"
      },
      "source": [
        "result = forest.predict(X_test)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "sns.set(\"poster\")\n",
        "sns.set_style('whitegrid')\n",
        "conf_mat = confusion_matrix(y_test, result)\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap = True)\n",
        "print(conf_mat)\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat, index = [i for i in \"12345\"],\n",
        "                  columns = [i for i in \"12345\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "\n",
        "sns.heatmap(df_cm,cmap=cmap, annot=True, fmt='g').set_title('Confusion Matrix para Modelo Random Forest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYDqG3ZXJiyO"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LogReg = LogisticRegression(max_iter = 10000)\n",
        "\n",
        "LogReg = LogReg.fit(X_train, y_train)\n",
        "\n",
        "print(LogReg)\n",
        "\n",
        "print(np.mean(cross_val_score(LogReg, data_features, labels, cv=10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKtowO5VJyKj"
      },
      "source": [
        "result_logreg = LogReg.predict(X_test)\n",
        "conf_mat = confusion_matrix(y_test, result_logreg)\n",
        "\n",
        "cmap = sns.diverging_palette(120, 50, as_cmap = True)\n",
        "print(conf_mat)\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat, index = [i for i in \"12345\"],\n",
        "                  columns = [i for i in \"12345\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm,cmap=cmap, annot=True, fmt='g').set_title('Confusion Matrix para Modelo Logistic Regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA1TBvQuQxoN"
      },
      "source": [
        "X = Rappi['content']\n",
        "y = Rappi['score']\n",
        "#Fazendo um novo split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, \n",
        "random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsO4j3M5REl0"
      },
      "source": [
        "cvec = CountVectorizer(max_features = 1000).fit(X_train)\n",
        "#Vamos chamar o primeiro train set de df_train\n",
        "df_train = pd.DataFrame(cvec.transform(X_train).todense(), columns = cvec.get_feature_names())\n",
        "df_test = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())\n",
        "print(df_train.shape)\n",
        "print(y_train.shape)\n",
        "print(df_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy0IUu3aRMr_"
      },
      "source": [
        "X = Rappi['reviewCreatedVersion'].apply(str)\n",
        "y = Rappi['score']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
        "cvec = CountVectorizer(max_features = 1000).fit(X_train)\n",
        "\n",
        "Version_train = pd.DataFrame(cvec.transform(X_train).todense(), columns = cvec.get_feature_names())\n",
        "Version_test = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpu88X3DRPIv"
      },
      "source": [
        "X = Rappi['thumbsUpCount'].apply(str)\n",
        "y = Rappi['score']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
        "cvec = CountVectorizer(max_features = 1000).fit(X_train)\n",
        "Thumbs_train = pd.DataFrame(cvec.transform(X_train).todense(), columns = cvec.get_feature_names())\n",
        "Thumbs_test = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjXOCQsSRbgf"
      },
      "source": [
        "X = Rappi['Sent'].apply(str)\n",
        "y = Rappi['score']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
        "cvec = CountVectorizer(max_features = 1000).fit(X_train)\n",
        "Sent_train = pd.DataFrame(cvec.transform(X_train).todense(), columns = cvec.get_feature_names())\n",
        "Sent_test = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBXQLe8eRuNo"
      },
      "source": [
        "X = Rappi['at'].apply(str)\n",
        "y = Rappi['score']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
        "cvec = CountVectorizer(max_features = 1000).fit(X_train)\n",
        "at_train = pd.DataFrame(cvec.transform(X_train).todense(), columns = cvec.get_feature_names())\n",
        "at_test = pd.DataFrame(cvec.transform(X_test).todense(), columns=cvec.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwmzJRAhR-Jd"
      },
      "source": [
        "train = pd.concat ([df_train, Sent_train, Thumbs_train, Version_train, at_train], axis = 1)\n",
        "test = pd.concat([df_test, Sent_test, Thumbs_test, Version_test, at_test], axis = 1)\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v-bNzDYSIEq"
      },
      "source": [
        "forest = RandomForestClassifier(n_estimators=10, n_jobs=4)\n",
        "forest = forest.fit(train, y_train)\n",
        "print(forest)\n",
        "print(np.mean(cross_val_score(forest,test, y_test, cv=10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INIa_BN8SRur"
      },
      "source": [
        "LogReg = LogisticRegression(max_iter = 10000)\n",
        "\n",
        "LogReg = LogReg.fit(train, y_train)\n",
        "\n",
        "print(LogReg)\n",
        "\n",
        "print(np.mean(cross_val_score(LogReg, test, y_test, cv=10)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}